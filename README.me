# **BC Data Pipeline**

---

## **ğŸ“– Project Summary**

This repository implements a fully automated, end-to-end data pipeline for Basket Craftâ€™s website session analytics. It extracts DecemberÂ 1â€“31,Â 2023 session data from an AWS RDS MySQL source using Python (Pandas & SQLAlchemy), loads it into a raw schema in AWS RDS Postgres, transforms the data through `staging` and `warehouse` layers with dbt in GitHub Codespaces, and visualizes key metrics in an interactive Looker Studio dashboard. CI/CD is orchestrated via GitHub Actions & Secrets, with version control managed in Git/GitHub.

---

## **ğŸ—‚ Repository Structure**

```text
BC-DATA-PIPELINE/
â”œâ”€â”€ .github/                          # GitHub Actions workflows
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ basket_craft_extract_load_raw.yml
â”œâ”€â”€ analyses/                         # Pipeline diagrams & ad-hoc queries
â”‚   â””â”€â”€ pipeline_diagram.png          # Data flow diagram
â”œâ”€â”€ dbt_basket_craft/                 # dbt project directory
â”‚   â”œâ”€â”€ dbt_project.yml               # dbt config
â”‚   â”œâ”€â”€ macros/                       # Custom dbt macros
â”‚   â”‚   â””â”€â”€ generate_schema_name.sql
â”‚   â”œâ”€â”€ models/                       # dbt models
â”‚   â”‚   â”œâ”€â”€ marts/                    # analytics marts
â”‚   â”‚   â”‚   â””â”€â”€ finance/
â”‚   â”‚   â”‚       â””â”€â”€ fct_finance_orders_daily.sql
â”‚   â”‚   â”œâ”€â”€ staging/                  # raw â†’ standardize
â”‚   â”‚   â”‚   â”œâ”€â”€ _sources.yml
â”‚   â”‚   â”‚   â””â”€â”€ stg_website_sessions.sql
â”‚   â”‚   â””â”€â”€ warehouse/                # aggregated fact tables
â”‚   â”‚       â””â”€â”€ fct_website_sessions_utm_source_daily.sql
â”‚   â””â”€â”€ profiles.yml                 # dbt connection settings
â”œâ”€â”€ elt/                              # Extract & Load scripts
â”‚   â”œâ”€â”€ basket_craft_orders_extract_load_raw.py
â”‚   â”œâ”€â”€ basket_craft_products_extract_load_raw.py
â”‚   â””â”€â”€ basket_craft_website_sessions_extract_load_raw.py
â”œâ”€â”€ logs/                             # ETL & dbt logs
â”œâ”€â”€ seeds/                            # dbt seed data
â”œâ”€â”€ snapshots/                        # dbt snapshots
â”œâ”€â”€ target/                           # Compiled dbt artifacts
â”œâ”€â”€ tests/                            # dbt tests
â”œâ”€â”€ .env                              # Local environment variables
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                         # Project documentation (this file)
â””â”€â”€ requirements.txt                  # Python dependencies
```

---

## **ğŸ“Š Data Pipeline Diagram**

![Pipeline Diagram](analyses/pipeline_diagram.png)
*Illustrates the full flow: AWS RDS MySQL â†’ Python ELT â†’ raw Postgres â†’ dbt staging & warehouse â†’ Looker Studio (with GitHub Actions & Secrets, Git/GitHub).*

---

## **ğŸš€ Getting Started**

### **1. Prerequisites**

* **PythonÂ 3.9+** & install dependencies:

  ```bash
  pip install -r requirements.txt
  ```
* **dbt Core** (Postgres adapter):

  ```bash
  pip install dbt-postgres
  ```
* **AWS RDS** MySQL & Postgres credentials in `.env` and `~/.dbt/profiles.yml`
* **Looker Studio** account for dashboard creation

---

### **2. Extract & Load (ELT)**

* **Script**: `elt/basket_craft_website_sessions_extract_load_raw.py`
* **Logic**:

  * Query `website_sessions` for `created_at BETWEEN '2023-12-01' AND '2023-12-31 23:59:59'`
  * Overwrite `raw.website_sessions` in Postgres
* **Run**:

  ```bash
  python elt/basket_craft_website_sessions_extract_load_raw.py
  ```
* **Automation**: `.github/workflows/basket_craft_extract_load_raw.yml` runs every 15Â minutes and supports manual dispatch

---

### **3. Transform with dbt**

1. **Configure** Postgres connection in `dbt_basket_craft/profiles.yml`
2. **Execute** from project root:

   ```bash
   cd dbt_basket_craft/
   dbt clean
   ```

dbt deps
dbt run
dbt test

```
3. **Layers**:
- **Staging** (`models/staging/stg_website_sessions.sql`):
  - `SELECT * FROM {{ source('basket_craft','website_sessions') }}`
  - Rename `created_at` â†’ `website_session_created_at`
  - Add `CURRENT_TIMESTAMP AS loaded_at`
- **Warehouse** (`models/warehouse/fct_website_sessions_utm_source_daily.sql`):
  - Daily grain: `DATE(website_session_created_at) AS website_session_day`
  - `COUNT(website_session_id) AS sessions`
  - `SUM(is_repeat_session)::INT AS repeat_sessions`
  - `ROUND((SUM(is_repeat_session)::NUMERIC / COUNT(*)) * 100, 2) AS repeat_sessions_pct`
  - Group by `website_session_day, utm_source`

---

### **4. Visualize in Looker Studio**
1. **Connect** Looker to Postgres and select `fct_website_sessions_utm_source_daily`
2. **Create** charts (enable cross-filtering except Scorecard, add date control):
- **Table**: Sessions per day
- **Heatmap**: % repeat sessions by UTM source
- **Scorecard**: Total sessions with prior-period comparison
- **Time series**: Daily sessions trend
- **Bar chart**: Sessions by UTM source
3. **Share**: Set dashboard to **Unlisted** and copy link below

---

## **ğŸ“ Dashboard**
**Unlisted Looker Studio Dashboard:**  
https://lookerstudio.google.com/reporting/36ac3c5d-9da3-47db-af7e-cce9fbedf65d

---

## **ğŸ“ Submission**
1. Commit & push all code, configs, ETL scripts, dbt models, and `analyses/pipeline_diagram.png`
2. Verify end-to-end: ELT â†’ raw â†’ dbt â†’ dashboard
3. Submit GitHub repository link on Brightspace by the deadline

---

## **ğŸ”– Technologies & Tools**
- **Data Infrastructure**: AWS RDS MySQL, AWS RDS Postgres
- **Development**: Python, Pandas, SQLAlchemy, SQL, GitHub Codespaces
- **Transform**: dbt Core (staging, warehouse, marts)
- **Automation**: GitHub Actions, GitHub Secrets
- **Visualization**: Looker Studio
- **Version Control**: Git, GitHub

```
