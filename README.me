# **BC Data Pipeline**

---

## **📖 Project Summary**

This repository implements a fully automated, end-to-end data pipeline for Basket Craft’s website session analytics. It extracts December 1–31, 2023 session data from an AWS RDS MySQL source using Python (Pandas & SQLAlchemy), loads it into a raw schema in AWS RDS Postgres, transforms the data through `staging` and `warehouse` layers with dbt in GitHub Codespaces, and visualizes key metrics in an interactive Looker Studio dashboard. CI/CD is orchestrated via GitHub Actions & Secrets, with version control managed in Git/GitHub.

---

## **🗂 Repository Structure**

```text
BC-DATA-PIPELINE/
├── .github/                          # GitHub Actions workflows
│   └── workflows/
│       └── basket_craft_extract_load_raw.yml
├── analyses/                         # Pipeline diagrams & ad-hoc queries
│   └── pipeline_diagram.png          # Data flow diagram
├── dbt_basket_craft/                 # dbt project directory
│   ├── dbt_project.yml               # dbt config
│   ├── macros/                       # Custom dbt macros
│   │   └── generate_schema_name.sql
│   ├── models/                       # dbt models
│   │   ├── marts/                    # analytics marts
│   │   │   └── finance/
│   │   │       └── fct_finance_orders_daily.sql
│   │   ├── staging/                  # raw → standardize
│   │   │   ├── _sources.yml
│   │   │   └── stg_website_sessions.sql
│   │   └── warehouse/                # aggregated fact tables
│   │       └── fct_website_sessions_utm_source_daily.sql
│   └── profiles.yml                 # dbt connection settings
├── elt/                              # Extract & Load scripts
│   ├── basket_craft_orders_extract_load_raw.py
│   ├── basket_craft_products_extract_load_raw.py
│   └── basket_craft_website_sessions_extract_load_raw.py
├── logs/                             # ETL & dbt logs
├── seeds/                            # dbt seed data
├── snapshots/                        # dbt snapshots
├── target/                           # Compiled dbt artifacts
├── tests/                            # dbt tests
├── .env                              # Local environment variables
├── .gitignore
├── README.md                         # Project documentation (this file)
└── requirements.txt                  # Python dependencies
```

---

## **📊 Data Pipeline Diagram**

![Pipeline Diagram](analyses/pipeline_diagram.png)
*Illustrates the full flow: AWS RDS MySQL → Python ELT → raw Postgres → dbt staging & warehouse → Looker Studio (with GitHub Actions & Secrets, Git/GitHub).*

---

## **🚀 Getting Started**

### **1. Prerequisites**

* **Python 3.9+** & install dependencies:

  ```bash
  pip install -r requirements.txt
  ```
* **dbt Core** (Postgres adapter):

  ```bash
  pip install dbt-postgres
  ```
* **AWS RDS** MySQL & Postgres credentials in `.env` and `~/.dbt/profiles.yml`
* **Looker Studio** account for dashboard creation

---

### **2. Extract & Load (ELT)**

* **Script**: `elt/basket_craft_website_sessions_extract_load_raw.py`
* **Logic**:

  * Query `website_sessions` for `created_at BETWEEN '2023-12-01' AND '2023-12-31 23:59:59'`
  * Overwrite `raw.website_sessions` in Postgres
* **Run**:

  ```bash
  python elt/basket_craft_website_sessions_extract_load_raw.py
  ```
* **Automation**: `.github/workflows/basket_craft_extract_load_raw.yml` runs every 15 minutes and supports manual dispatch

---

### **3. Transform with dbt**

1. **Configure** Postgres connection in `dbt_basket_craft/profiles.yml`
2. **Execute** from project root:

   ```bash
   cd dbt_basket_craft/
   dbt clean
   ```

dbt deps
dbt run
dbt test

```
3. **Layers**:
- **Staging** (`models/staging/stg_website_sessions.sql`):
  - `SELECT * FROM {{ source('basket_craft','website_sessions') }}`
  - Rename `created_at` → `website_session_created_at`
  - Add `CURRENT_TIMESTAMP AS loaded_at`
- **Warehouse** (`models/warehouse/fct_website_sessions_utm_source_daily.sql`):
  - Daily grain: `DATE(website_session_created_at) AS website_session_day`
  - `COUNT(website_session_id) AS sessions`
  - `SUM(is_repeat_session)::INT AS repeat_sessions`
  - `ROUND((SUM(is_repeat_session)::NUMERIC / COUNT(*)) * 100, 2) AS repeat_sessions_pct`
  - Group by `website_session_day, utm_source`

---

### **4. Visualize in Looker Studio**
1. **Connect** Looker to Postgres and select `fct_website_sessions_utm_source_daily`
2. **Create** charts (enable cross-filtering except Scorecard, add date control):
- **Table**: Sessions per day
- **Heatmap**: % repeat sessions by UTM source
- **Scorecard**: Total sessions with prior-period comparison
- **Time series**: Daily sessions trend
- **Bar chart**: Sessions by UTM source
3. **Share**: Set dashboard to **Unlisted** and copy link below

---

## **📎 Dashboard**
**Unlisted Looker Studio Dashboard:**  
https://lookerstudio.google.com/reporting/36ac3c5d-9da3-47db-af7e-cce9fbedf65d

---

## **📝 Submission**
1. Commit & push all code, configs, ETL scripts, dbt models, and `analyses/pipeline_diagram.png`
2. Verify end-to-end: ELT → raw → dbt → dashboard
3. Submit GitHub repository link on Brightspace by the deadline

---

## **🔖 Technologies & Tools**
- **Data Infrastructure**: AWS RDS MySQL, AWS RDS Postgres
- **Development**: Python, Pandas, SQLAlchemy, SQL, GitHub Codespaces
- **Transform**: dbt Core (staging, warehouse, marts)
- **Automation**: GitHub Actions, GitHub Secrets
- **Visualization**: Looker Studio
- **Version Control**: Git, GitHub

```
